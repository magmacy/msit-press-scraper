import os
import sys
import time
import logging
import argparse
import requests
import pandas as pd
from bs4 import BeautifulSoup
from urllib.parse import urljoin, unquote
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from tqdm import tqdm
import re
from datetime import datetime

import config
import utils
import migrate_folders

# ë¡œê¹… ì„¤ì •
def setup_logging():
    os.makedirs(config.LOG_DIR, exist_ok=True)
    
    log_file = os.path.join(config.LOG_DIR, f"scraper_{config.TODAY_STR}.log")
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    return logging.getLogger(__name__)

logger = setup_logging()

class PressReleaseScraper:
    def __init__(self, year=config.TARGET_YEAR, output_file=config.EXCEL_PATH):
        self.target_year = year
        self.output_file = output_file
        self.session = self._setup_session()
        self.collected_data = []
        self.seen_ids = set()
        
        # ì´ì–´ë°›ê¸°: ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ID ë¡œë“œ
        if os.path.exists(self.output_file):
            try:
                df = pd.read_excel(self.output_file)
                if 'ë²ˆí˜¸' in df.columns:
                    self.seen_ids = set(df['ë²ˆí˜¸'].astype(str).tolist())
                    logger.info(f"ê¸°ì¡´ ë°ì´í„° {len(self.seen_ids)}ê±´ ë¡œë“œ ì™„ë£Œ. ì¤‘ë³µ ìˆ˜ì§‘ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            except Exception as e:
                logger.warning(f"ê¸°ì¡´ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")

    def _setup_session(self):
        """ì•ˆì •ì ì¸ ë„¤íŠ¸ì›Œí¬ ìš”ì²­ì„ ìœ„í•œ ì„¸ì…˜ ì„¤ì •"""
        session = requests.Session()
        session.headers.update(config.HEADERS)
        
        retry_strategy = Retry(
            total=config.MAX_RETRIES,
            backoff_factor=config.BACKOFF_FACTOR,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["HEAD", "GET", "OPTIONS"]
        )
        
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("https://", adapter)
        session.mount("http://", adapter)
        return session

    def download_attachment(self, url, folder_name):
        """ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        try:
            response = self.session.get(url, stream=True, timeout=config.TIMEOUT)
            response.raise_for_status()
            
            filename = ""
            # Content-Disposition í—¤ë” í™•ì¸
            if "Content-Disposition" in response.headers:
                cd = response.headers["Content-Disposition"]
                # RFC 5987: filename*=UTF-8''EncodedString
                matches = re.findall(r"filename\*=UTF-8''(.+)", cd)
                if matches:
                    filename = unquote(matches[0])
                else:
                    # filename="Name"
                    matches = re.findall(r'filename="([^"]+)"', cd)
                    if matches:
                        filename = unquote(matches[0])
            
            # í—¤ë”ì—ì„œ ì‹¤íŒ¨í–ˆê±°ë‚˜ ì—†ëŠ” ê²½ìš° URLì—ì„œ ì¶”ì¶œ
            if not filename:
                # ë¦¬ë‹¤ì´ë ‰íŠ¸ëœ ìµœì¢… URL ê¸°ì¤€
                if response.history:
                    url = response.url
                filename = unquote(os.path.basename(url))
            
            # íŒŒì¼ëª… ì •ì œ (íŠ¹ìˆ˜ë¬¸ì ì œê±°)
            filename = re.sub(r'[\\/*?:"<>|]', "", filename)
            
            # íŒŒì¼ëª… ê¸¸ì´ ì œí•œ (Windows MAX_PATH ê³ ë ¤, 100ìë¡œ ì œí•œ)
            name, ext = os.path.splitext(filename)
            if len(name) > 80:
                name = name[:80]
            filename = f"{name}{ext}"
            
            save_dir = os.path.join(config.DOWNLOAD_DIR, folder_name)
            os.makedirs(save_dir, exist_ok=True)
            
            file_path = os.path.join(save_dir, filename)
            
            # ì´ë¯¸ ìˆìœ¼ë©´ ìŠ¤í‚µ
            if os.path.exists(file_path):
                return filename, file_path

            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
                    
            return filename, file_path
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ({url}): {e}")
            return None, None

    def _extract_dates_from_script(self, soup):
        """
        í˜ì´ì§€ ë‚´ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ë™ì ìœ¼ë¡œ í• ë‹¹ë˜ëŠ” ë‚ ì§œ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        í˜•ì‹: $('#td_'+'REG_DT'+'_0').html('Feb 6, 2026');
        ë°˜í™˜ê°’: {index: date_str} ë”•ì…”ë„ˆë¦¬
        """
        date_map = {}
        scripts = soup.find_all('script')
        target_script = None

        for script in scripts:
            if script.string and "$('#td_'" in script.string and "REG_DT" in script.string:
                target_script = script.string
                break
        
        if target_script:
            # ì •ê·œì‹ìœ¼ë¡œ ì¸ë±ìŠ¤ì™€ ë‚ ì§œ ì¶”ì¶œ
            # ì˜ˆ: $('#td_'+'REG_DT'+'_0').html('Feb 6, 2026');
            pattern = re.compile(r"\$\('#td_'\s*\+\s*'REG_DT'\s*\+\s*'_(\d+)'\)\.html\('([^']+)'\)")
            matches = pattern.findall(target_script)
            
            for idx, date_str in matches:
                # utils.normalize_dateë¡œ ë‚ ì§œ í‘œì¤€í™”
                date_map[int(idx)] = utils.normalize_date(date_str) 

        return date_map

    def get_list_page(self, page):
        """ëª©ë¡ í˜ì´ì§€ íŒŒì‹±"""
        url = f"{config.LIST_URL}&pageIndex={page}"
        try:
            response = self.session.get(url, timeout=config.TIMEOUT)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
            script_dates = self._extract_dates_from_script(soup)
            
            # ë§í¬ ì°¾ê¸° ë° ì¸ë±ìŠ¤ ë§¤í•‘
            # ìŠ¤í¬ë¦½íŠ¸ì˜ ì¸ë±ìŠ¤(_0, _1...)ëŠ” .board_list ë‚´ì˜ ìˆœì„œì™€ ì¼ì¹˜í•¨
            links = soup.select('.board_list .toggle > a[onclick^="fn_detail"]')

            items = []
            seen_page_ids = set()

            for idx, link in enumerate(links):
                onclick = link['onclick']
                match = re.search(r"fn_detail\((\d+)\)", onclick)
                if not match:
                    continue
                ntt_id = match.group(1)

                if ntt_id in seen_page_ids:
                    continue
                seen_page_ids.add(ntt_id)

                # ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°: ìŠ¤í¬ë¦½íŠ¸ ë§¤í•‘ ìš°ì„ 
                date_str = script_dates.get(idx, "")
                
                # HTML ë°±ì—… (í˜¹ì‹œ ëª¨ë¥¼ ìƒí™© ëŒ€ë¹„)
                if not date_str:
                    li = link.find_parent('div', class_='toggle')
                    if li:
                        date_div = li.find('div', class_='date')
                        if date_div:
                            date_str = date_div.get_text(strip=True).replace('ë“±ë¡ì¼', '').strip()

                if not date_str:
                    date_str = datetime.now().strftime("%Y-%m-%d")

                items.append((ntt_id, date_str))

            return items
            
        except Exception as e:
            logger.error(f"ëª©ë¡ í˜ì´ì§€ {page} ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []

    def get_detail_page(self, ntt_id, date_str):
        """ìƒì„¸ í˜ì´ì§€ íŒŒì‹±"""
        url = f"{config.BASE_URL}/bbs/view.do?sCode=user&mPid=208&mId=307&bbsSeqNo=94&nttSeqNo={ntt_id}"
        
        try:
            response = self.session.get(url, timeout=config.TIMEOUT)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ì œëª©
            title_elem = soup.select_one('.view_head h2')
            title = utils.clean_text(title_elem.get_text()) if title_elem else f"ì œëª©ì—†ìŒ_{ntt_id}"
            
            # ë¶€ì„œ
            dept = ""
            for dt in soup.select('.tit_con dt'):
                if "ë¶€ì„œ" in dt.get_text():
                    dd = dt.find_next_sibling('dd')
                    if dd:
                        dept = utils.clean_text(dd.get_text())
                    break
            
            # ë³¸ë¬¸
            content_div = soup.select_one('.board_notcon') or soup.select_one('.board_pc')
            content = utils.clean_text(content_div.get_text()) if content_div else ""
            
            # ìš”ì•½
            summary = utils.summarize_text(content)
            
            # ì²¨ë¶€íŒŒì¼ ì²˜ë¦¬
            attachments = []
            file_paths = []
            
            # JS ë‹¤ìš´ë¡œë“œ íŒ¨í„´: fn_download('atch_no', 'file_ord', 'ext')
            download_scripts = re.findall(r"fn_download\('(\d+)',\s*'(\d+)',\s*'([^']+)'\)", response.text)
            downloaded_set = set()
            
            title_clean = re.sub(r'[\\\\/*?:\"<>|]', '', title)
            folder_name = f"{date_str}_{title_clean[:30].strip()}"
            
            for atch_no, file_ord, _ in download_scripts:
                down_url = f"{config.BASE_URL}/ssm/file/fileDown.do?atchFileNo={atch_no}&fileOrd={file_ord}&fileBtn=A"
                if down_url in downloaded_set:
                    continue
                    
                fname, fpath = self.download_attachment(down_url, folder_name)
                if fname:
                    attachments.append(fname)
                    # ì ˆëŒ€ ê²½ë¡œë¥¼ ìƒëŒ€ ê²½ë¡œë¡œ ë³€í™˜ (í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€)
                    rel_path = os.path.relpath(fpath, config.BASE_DIR)
                    file_paths.append(rel_path)
                    downloaded_set.add(down_url)
            
            # ì²¨ë¶€íŒŒì¼ ê²½ë¡œë¥¼ í•˜ì´í¼ë§í¬ ìˆ˜ì‹ìœ¼ë¡œ ë³€í™˜
            # ì—‘ì…€ íŒŒì¼(data í´ë”) ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œë¡œ ë³€í™˜ í•„ìš”
            # rel_pathëŠ” í˜„ì¬ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ì„ (downloads/...)
            # data í´ë” ë‚´ë¶€ì—ì„œ downloadsë¡œ ê°€ë ¤ë©´ ../downloads/...
            
            if file_paths:
                # ì²« ë²ˆì§¸ íŒŒì¼ ê¸°ì¤€ í´ë” ê²½ë¡œ
                # file_paths[0] = downloads\folder\file
                folder_path_rel_project = os.path.dirname(file_paths[0]) # downloads\folder
                folder_path_rel_excel = os.path.join("..", folder_path_rel_project)
                
                display_text = f"ğŸ“‚ í´ë” ì—´ê¸° ({', '.join(attachments)})"
                if len(display_text) > 200:
                    display_text = f"ğŸ“‚ í´ë” ì—´ê¸° ({len(attachments)}ê°œ íŒŒì¼)"
                    
                hyperlink = f'=HYPERLINK("{folder_path_rel_excel}", "{display_text}")'
                
                # file_paths ë¦¬ìŠ¤íŠ¸ ëŒ€ì‹  ìˆ˜ì‹ ë¬¸ìì—´ ì €ì¥
                # ì£¼ì˜: ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ˆë¼ ë¬¸ìì—´ë¡œ ì €ì¥ë¨
                final_paths = hyperlink
            else:
                final_paths = ""

            return {
                'ë²ˆí˜¸': ntt_id,
                'ì œëª©': title,
                'ë“±ë¡ì¼': date_str,
                'ë¶€ì„œ': dept,
                'ìƒì„¸URL': url,
                'ë³¸ë¬¸': content,
                'í•µì‹¬ìš”ì•½': summary,
                'ì²¨ë¶€íŒŒì¼ëª©ë¡': ", ".join(attachments),
                'ì²¨ë¶€íŒŒì¼ê²½ë¡œ': final_paths
            }
            
        except Exception as e:
            logger.error(f"ìƒì„¸ í˜ì´ì§€ {ntt_id} íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None

    def save_data(self):
        """ë°ì´í„° ì €ì¥"""
        if not self.collected_data:
            return

        new_df = pd.DataFrame(self.collected_data)
        
        if os.path.exists(self.output_file):
            try:
                old_df = pd.read_excel(self.output_file)
                # ë²ˆí˜¸ ê¸°ì¤€ ì¤‘ë³µ ì œê±° í›„ ë³‘í•©
                combined_df = pd.concat([old_df, new_df]).drop_duplicates(subset=['ë²ˆí˜¸'], keep='last')
                try:
                    combined_df.to_excel(self.output_file, index=False, engine='openpyxl')
                except PermissionError:
                    # íŒŒì¼ì´ ì—´ë ¤ìˆì–´ì„œ ì €ì¥ì´ ì•ˆë˜ëŠ” ê²½ìš°
                    new_filename = self.output_file.replace(".xlsx", f"_backup_{datetime.now().strftime('%H%M%S')}.xlsx")
                    logger.warning(f"íŒŒì¼ì´ ì—´ë ¤ìˆì–´ ì €ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë°±ì—… íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤: {new_filename}")
                    combined_df.to_excel(new_filename, index=False, engine='openpyxl')
            except Exception as e:
                logger.error(f"ë°ì´í„° ë³‘í•© ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
                # ë³‘í•© ì‹¤íŒ¨ ì‹œ í˜„ì¬ ë°ì´í„°ë¼ë„ ë”°ë¡œ ì €ì¥
                new_filename = self.output_file.replace(".xlsx", f"_partial_{datetime.now().strftime('%H%M%S')}.xlsx")
                new_df.to_excel(new_filename, index=False, engine='openpyxl')
        else:
            try:
                new_df.to_excel(self.output_file, index=False, engine='openpyxl')
            except PermissionError:
                new_filename = self.output_file.replace(".xlsx", f"_new_{datetime.now().strftime('%H%M%S')}.xlsx")
                logger.warning(f"íŒŒì¼ì´ ì—´ë ¤ìˆì–´ ì €ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìƒˆ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤: {new_filename}")
                new_df.to_excel(new_filename, index=False, engine='openpyxl')
            
        logger.info(f"ë°ì´í„° ì €ì¥ ì™„ë£Œ: {self.output_file}")
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        self.collected_data = []

    def run(self, start_page=1, test_mode=False):
        logger.info(f"ìˆ˜ì§‘ ì‹œì‘ (ëŒ€ìƒ ì—°ë„: {self.target_year}ë…„ ì´ìƒ)")
        if test_mode:
            logger.info(">> í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ìˆ˜ì§‘ ê±´ìˆ˜ê°€ 5ê±´ì— ë„ë‹¬í•˜ë©´ ì¢…ë£Œí•©ë‹ˆë‹¤.")

        os.makedirs(config.DATA_DIR, exist_ok=True)
        os.makedirs(config.DOWNLOAD_DIR, exist_ok=True)
        
        page = start_page
        stop_flag = False
        total_collected = 0
        
        # tqdm ì„¤ì •
        pbar = tqdm(desc="í˜ì´ì§€ ìˆ˜ì§‘", unit="page")
        
        while not stop_flag:
            pbar.set_description(f"Page {page}")
            items = self.get_list_page(page)
            
            if not items:
                logger.info("ë” ì´ìƒ ê²Œì‹œê¸€ì´ ì—†ê±°ë‚˜ íŒŒì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                break
                
            new_page_items = 0
            
            for idx, (ntt_id, date_str) in enumerate(items):
                # ì—°ë„ ì²´í¬
                dt = utils.parse_date(date_str)
                if dt and dt.year < self.target_year:
                    logger.info(f"ê¸°ì¤€ ì—°ë„({self.target_year}) ì´ì „ ë°ì´í„° ë„ë‹¬ ({date_str}). ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    stop_flag = True
                    break
                
                # ì¤‘ë³µ ì²´í¬
                if str(ntt_id) in self.seen_ids and not test_mode:
                    continue
                    
                # ìƒì„¸ ìˆ˜ì§‘
                # ì§„í–‰ ìƒí™© ë¡œê·¸ (í„°ë¯¸ë„ ì¶œë ¥ìš©)
                tqdm.write(f"  - [{idx+1}/{len(items)}] ìƒì„¸ ìˆ˜ì§‘ ì¤‘: {ntt_id} ({date_str})")
                
                data = self.get_detail_page(ntt_id, date_str)
                if data:
                    self.collected_data.append(data)
                    self.seen_ids.add(str(ntt_id))
                    new_page_items += 1
                    total_collected += 1
                    
                    tqdm.write(f"    Target: {data['ì œëª©'][:30]}...")

                    if test_mode and total_collected >= 5:
                        logger.info("í…ŒìŠ¤íŠ¸ ëª©í‘œ ë‹¬ì„± (5ê±´). ì¢…ë£Œí•©ë‹ˆë‹¤.")
                        stop_flag = True
                        break
                    
                time.sleep(1) # ë¶€í•˜ ì¡°ì ˆ
            
            # í˜ì´ì§€ ë‹¨ìœ„ ì €ì¥
            if self.collected_data:
                self.save_data()
                
            if new_page_items == 0 and not stop_flag and not test_mode:
                logger.info(f"í˜ì´ì§€ {page}ì˜ ëª¨ë“  ë°ì´í„°ê°€ ì´ë¯¸ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤. (ì¤‘ë³µ)")
                # ì—°ì†ìœ¼ë¡œ ì¤‘ë³µì´ë©´ ì¢…ë£Œí•˜ëŠ” ë¡œì§ë„ ê³ ë ¤ ê°€ëŠ¥í•˜ì§€ë§Œ ì¼ë‹¨ ì§„í–‰
                
            page += 1
            pbar.update(1)
            
            if test_mode and stop_flag:
                break
            
        pbar.close()
        logger.info("ìˆ˜ì§‘ ì¢…ë£Œ")

def main():
    parser = argparse.ArgumentParser(description="ê³¼í•™ê¸°ìˆ ì •ë³´í†µì‹ ë¶€ ë³´ë„ìë£Œ ìŠ¤í¬ë˜í¼")
    parser.add_argument("--page", type=int, default=1, help="ì‹œì‘ í˜ì´ì§€ ë²ˆí˜¸")
    parser.add_argument("--year", type=int, default=config.TARGET_YEAR, help="ìˆ˜ì§‘ ê¸°ì¤€ ì—°ë„ (ì´í›„ ë°ì´í„° ìˆ˜ì§‘)")
    parser.add_argument("--test", action="store_true", help="í…ŒìŠ¤íŠ¸ ëª¨ë“œ (1í˜ì´ì§€ë§Œ ìˆ˜ì§‘í•˜ê³  ì¢…ë£Œ)")
    
    args = parser.parse_args()
    
    # ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ
    if args.year:
        config.TARGET_YEAR = args.year
        
    scraper = PressReleaseScraper(year=config.TARGET_YEAR)
    
    if args.test:
        # í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì‹œ íŒŒì¼ëª… ë³€ê²½ (ë®ì–´ì“°ê¸° ë°©ì§€)
        scraper.output_file = config.EXCEL_PATH.replace(".xlsx", "_test.xlsx")
        
    scraper.run(start_page=args.page, test_mode=args.test)
    
    # ìˆ˜ì§‘ ì™„ë£Œ í›„ í´ë”ëª… ë³€ê²½ (ë§ˆì´ê·¸ë ˆì´ì…˜) ìë™ ì‹¤í–‰
    if not args.test: # í…ŒìŠ¤íŠ¸ ëª¨ë“œê°€ ì•„ë‹ ë•Œë§Œ ì‹¤í–‰í•˜ê±°ë‚˜, í•„ìš”ì— ë”°ë¼ ì¡°ì •
        logger.info("í´ë”ëª… ë§ˆì´ê·¸ë ˆì´ì…˜(ë‚ ì§œ ìˆ˜ì •) ì‹œì‘...")
        try:
            migrate_folders.migrate_folders()
        except Exception as e:
            logger.error(f"ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰ ì¤‘ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    main()
